{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b7d4c4-98f2-43eb-85ba-05c3fc84c760",
   "metadata": {},
   "source": [
    "**************Final List**************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d13ee548-abb8-4a7b-b8ac-239c3c2e7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading for Page No 0 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 10 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 20 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 30 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 40 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 50 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 60 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 70 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 80 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 90 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 100 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 110 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 120 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 130 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 140 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 150 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 160 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 170 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 180 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 190 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 200 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 210 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 220 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 230 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 240 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 250 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 260 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 270 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 280 Starts...>>>>..>>>>>.>>>\n",
      "Data Loading for Page No 290 Starts...>>>>..>>>>>.>>>\n",
      "290 Pages Data Successfully saved to Excel file!\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import xlwt\n",
    "\n",
    "wb = xlwt.Workbook()\n",
    "ws = wb.add_sheet('Jobs Data')\n",
    "ws.write(0, 0, 'Job Title')\n",
    "ws.write(0, 1, 'Company Name')\n",
    "ws.write(0, 2, 'Location')\n",
    "ws.write(0, 3, 'Job Salary')\n",
    "ws.write(0, 4, 'Job Type')\n",
    "ws.write(0, 5, 'Job Description')\n",
    "\n",
    "\n",
    "def scrape_jobs():\n",
    "    base_url = \"https://indeed.com/\"\n",
    "    job_search = \"AI\"\n",
    "    row = 1\n",
    "    for i in range(0, 300, 10):\n",
    "        print(f\"Data Loading for Page No {i} Starts...>>>>..>>>>>.>>>\")\n",
    "        url = f\"{base_url}jobs?q={job_search}&start={i}&l=&from=searchOnHP\"\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        response = scraper.get(url)\n",
    "        bs = BeautifulSoup(response.text,\"html.parser\") \n",
    "        job_list = bs.find('ul',{'class':'css-zu9cdh eu4oa1w0'})\n",
    "        jobs = job_list.find_all(\"div\",{'class':\"job_seen_beacon\"})\n",
    "        for job in jobs:\n",
    "            title = job.find('h2', {'class': 'jobTitle'}).text\n",
    "            if title:\n",
    "                ws.write(row, 0, title.strip())\n",
    "            \n",
    "            company = job.find('span', {'class': 'css-1x7z1ps eu4oa1w0'}).text\n",
    "            if company:\n",
    "                ws.write(row, 1, company.strip())\n",
    "            \n",
    "            location = job.find('div', {'class': 'css-t4u72d eu4oa1w0'}).text\n",
    "            if location:\n",
    "                 ws.write(row, 2, location.strip())\n",
    "            \n",
    "            salary = job.find('div', {'class': 'css-1ihavw2 eu4oa1w0'})\n",
    "            if salary:\n",
    "                ws.write(row, 3, salary.text.strip())\n",
    "            \n",
    "            job_type = job.find('div', {'class': 'css-1ihavw2 eu4oa1w0'})\n",
    "            if job_type:\n",
    "                ws.write(row, 4, job_type.text.strip())\n",
    "            \n",
    "            job_dis = job.find('div', {'class': 'job-snippet'})\n",
    "            # if job_dis:\n",
    "            #     description = job_dis.find('li').text\n",
    "            #     ws.write(row, 5, description.strip())\n",
    "            if job_dis:\n",
    "                li_tag = job_dis.find('li')\n",
    "                if li_tag:\n",
    "                    description = li_tag.get_text(strip=True)\n",
    "                    ws.write(row, 5, description.strip())\n",
    "            \n",
    "            row += 1\n",
    "\n",
    "    wb.save(\"Jobs_Data.xls\")\n",
    "    print(f\"{i} Pages Data Successfully saved to Excel file!\")\n",
    "    print(\"*\" * 100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_jobs()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
